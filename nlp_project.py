import os

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from dotenv import load_dotenv
import torch

CLRS_TEXT_FIELDS = ["question", "answer", "algo_name"]
LLAMA_MODEL_NAME = "meta-llama/Meta-Llama-3-8B"
GEMMA_MODEL_NAME = "google/gemma-2b"

import datetime

def special_print(value, log_file='log.txt'):
    """
    Prints the given value and writes it into the given log file with a timestamp.

    Parameters:
    - value: The value to be logged (will be converted to string).
    - log_file: The path to the log file where the value will be stored.
    """
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_entry = f"[{timestamp}] {value}"
    
    # Print to console
    print(log_entry)
    
    # Append to log file
    try:
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry + '\n')
    except Exception as e:
        print(f"[ERROR] Could not write to log file: {e}")

def generate_prompt(data) -> str:
    algo_name = data.get("algo_name", "").strip()
    raw_question = data.get("question", "").strip()
    answer = data.get("answer", "").strip()

    # Remove the 'algo_name:' prefix from question if it exists
    prefix = f"{algo_name}:"
    if raw_question.startswith(prefix):
        question = raw_question[len(prefix):].lstrip()
    else:
        question = raw_question

    prompt = f"""You are an expert in algorithms and trace analysis.
    Given the following trace prompt for the `{algo_name}` algorithm, continue the trace or output the correct trace result.

    ### Algorithm: {algo_name}
    ### Input and Initial Trace:
    {question}

    ### Expected Output Format:
    Continue or complete the trace output as shown in prior examples.

    ### Answer:
    """

    return prompt


def get_CLRS_dataset(split="train"):
    """ 
    Train dataset is pre generated by the authors of the CLRS-Text paper.
    Test dataset is also generated but contains  5 different test splits,
    each split is generated with a different random seed.

    dataset structure:
    {
        "question": str, 
        "answer": str,
        "algo_name": str,
    """
    if split not in ["train", "test"]:
        raise ValueError("Split must be either 'train' or 'test'.")
    if split == "train":
        dataset = load_dataset("tomg-group-umd/CLRS-Text-train", split="train")
    else:
        dataset = load_dataset("tomg-group-umd/CLRS-Text-test")
    return dataset

def get_iterable_corpus(dataset, dataset_text_fields=CLRS_TEXT_FIELDS):
    for item in dataset:
        yield " ".join([item[field] for field in dataset_text_fields if field in item])

def train_tokenizer(pretrained_tokenizer_name, dataset):
    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_name)
    tokenizer.train_new_from_iterator(get_iterable_corpus(dataset), vocab_size=tokenizer.vocab_size)
    return tokenizer

def get_llama_answer(model,tokenizer, prompt) -> str:
    # Tokenize and move to the modelâ€™s device
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Inference without gradient tracking (saves memory)
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_new_tokens=200,
            do_sample=False,
            temperature=0.0,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id, # Set pad token id to eos token id
            attention_mask=inputs["attention_mask"] # Pass attention mask
        )

    # Decode and return only new text (not the prompt)
    # Decode the generated tokens, skipping the input tokens
    generated_tokens = outputs[0, len(inputs["input_ids"][0]):]
    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    print("Full output:", tokenizer.decode(outputs[0], skip_special_tokens=True)) # Print full output for debugging
    return answer.strip()



def main():
    print("Starting")

    print("Using CUDA:", torch.cuda.is_available())
    print("GPU Device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None")

    log_path = "log.txt"
    load_dotenv()  # Loads variables from .env into os.environ

    # Assuming HF_TOKEN is set in the environment variables
    assert("HF_TOKEN" in os.environ), "HF_TOKEN environment variable is not set."
    HF_TOKEN = os.getenv("HF_TOKEN")

    train_dataset = get_CLRS_dataset("train")
    test_dataset = get_CLRS_dataset("test")

    special_print("NEW RUN\n", log_path)

    special_print(train_dataset, log_path)
    special_print(train_dataset[1], log_path)
    special_print(train_dataset[2], log_path)
    special_print(train_dataset[3], log_path)
    special_print(train_dataset[4], log_path)

    special_print(train_dataset, log_path)
    special_print(train_dataset[1], log_path)

    prompt = generate_prompt(train_dataset[1])
    print("Generated prompt:\n", prompt)

    tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME, token=HF_TOKEN)
    model = AutoModelForCausalLM.from_pretrained(
        LLAMA_MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
        token=HF_TOKEN
    )
    model.eval()

    print("Running LLaMA...")
    answer = get_llama_answer(model, tokenizer, prompt)
    print("LLaMA answer:\n", answer)


if __name__ == "__main__":
    main()