import os

from datasets import load_dataset
from transformers import AutoTokenizer


CLRS_TEXT_FIELDS = ["question", "answer", "algo_name"]
LLAMA_MODEL_NAME = "meta-llama/Meta-Llama-3-8B"
GEMMA_MODEL_NAME = "google/gemma-2b"



def get_CLRS_dataset(split="train"):
    """ 
    Train dataset is pre generated by the authors of the CLRS-Text paper.
    Test dataset is also generated but contains  5 different test splits,
    each split is generated with a different random seed.

    dataset structure:
    {
        "question": str, 
        "answer": str,
        "algo_name": str,
    """
    if split not in ["train", "test"]:
        raise ValueError("Split must be either 'train' or 'test'.")
    if split == "train":
        dataset = load_dataset("tomg-group-umd/CLRS-Text-train", split="train")
    else:
        dataset = load_dataset("tomg-group-umd/CLRS-Text-test")
    return dataset

def get_iterable_corpus(dataset, dataset_text_fields=CLRS_TEXT_FIELDS):
    for item in dataset:
        yield " ".join([item[field] for field in dataset_text_fields if field in item])

def train_tokenizer(pretrained_tokenizer_name, dataset):
    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_name)
    tokenizer.train_new_from_iterator(get_iterable_corpus(dataset), vocab_size=tokenizer.vocab_size)
    return tokenizer


def main():
    # Assuming HF_TOKEN is set in the environment variables
    assert("HF_TOKEN" in os.environ), "HF_TOKEN environment variable is not set."

    train_dataset = get_CLRS_dataset("train")
    test_dataset = get_CLRS_dataset("test")
    
    # print(f"Train dataset size: {len(train_dataset)}")
    # print(f"Test dataset size: {len(test_dataset)}")
    # print("Sample:", train_dataset[0])
    text = get_iterable_corpus(train_dataset).__next__()
    print("Sample text:", text)

    print("LLama tokenizer:")
    tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)
    print("Original tokenizer vocab size:", tokenizer.vocab_size)
    tokens = tokenizer.tokenize(text)
    print("Sample tokenized:", tokens)

    print("Gamma tokenizer:")
    tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_NAME)
    print("Original tokenizer vocab size:", tokenizer.vocab_size)
    tokens = tokenizer.tokenize(text)
    print("Sample tokenized:", tokens)

if __name__ == "__main__":
    main()