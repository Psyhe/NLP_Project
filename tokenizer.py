import os

from datasets import load_dataset
from transformers import AutoTokenizer
from tokenizers import pre_tokenizers
from tokenizers.pre_tokenizers import Digits


CLRS_TEXT_FIELDS = ["question", "answer", "algo_name"]
LLAMA_MODEL_NAME = "meta-llama/Meta-Llama-3-8B"
GEMMA_MODEL_NAME = "google/gemma-2b"



def get_CLRS_dataset(split="train"):
    """ 
    Train dataset is pre generated by the authors of the CLRS-Text paper.
    Test dataset is also generated but contains  5 different test splits,
    each split is generated with a different random seed.

    dataset structure:
    {
        "question": str, 
        "answer": str,
        "algo_name": str,
    """
    if split not in ["train", "test"]:
        raise ValueError("Split must be either 'train' or 'test'.")
    if split == "train":
        dataset = load_dataset("tomg-group-umd/CLRS-Text-train", split="train")
    else:
        dataset = load_dataset("tomg-group-umd/CLRS-Text-test")
    return dataset

def get_iterable_corpus(dataset, dataset_text_fields=CLRS_TEXT_FIELDS):
    return (
        [" ".join([dataset[i+j][field] for field in dataset_text_fields]).replace("\n", " ").replace("_", " ") for j in range(1000)]
        for i in range(0, len(dataset)//10, 1000)
    )
    

def train_tokenizer_add_diff(pretrained_tokenizer_name, dataset, path = "tmp/new_tokenizer"):
    if path is not None:
        os.makedirs(path, exist_ok=True)

    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_name)
    new_tokenizer = tokenizer.train_new_from_iterator(get_iterable_corpus(dataset), vocab_size=1000)


    diff_tokens = set(new_tokenizer.vocab).difference(tokenizer.vocab)
    added_tokens = tokenizer.add_tokens(list(diff_tokens))
    print(f"Added {added_tokens} new tokens to the tokenizer.")

    if path is not None:
        tokenizer.save_pretrained(path)

    return tokenizer


def train_tokenizer_remove_numbers(pretrained_tokenizer_name, dataset, path = "tmp/new_tokenizer_without_numbers"):
    if path is not None:
        os.makedirs(path, exist_ok=True)

    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_name)
    pre_tokenizer = pre_tokenizers.Sequence([Digits(individual_digits=True), tokenizer._tokenizer.pre_tokenizer])
    tokenizer._tokenizer.pre_tokenizer = pre_tokenizer

    if path is not None:
        tokenizer.save_pretrained(path)

    return tokenizer


def load_tokenizer(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Tokenizer path '{path}' does not exist.")
    return AutoTokenizer.from_pretrained(path)

def test_tokenizer(tokenizer, text, name = None):
    if name is not None:
        print(f"Testing tokenizer: {name} with vocab size {len(tokenizer.vocab)}")

    tokens = tokenizer.tokenize(text)
    print("Tokenized text:", tokens)


def main():
    # Assuming HF_TOKEN is set in the environment variables
    assert("HF_TOKEN" in os.environ), "HF_TOKEN environment variable is not set."

    train_dataset = get_CLRS_dataset("train")
    test_dataset = get_CLRS_dataset("test")
    text =get_iterable_corpus(test_dataset["test_1"], CLRS_TEXT_FIELDS).__next__()[0]

    # new_tokenizer = train_tokenizer_add_diff(LLAMA_MODEL_NAME, train_dataset, path="tmp/new_tokenizer")
    # new_tokenizer = train_tokenizer_remove_numbers(LLAMA_MODEL_NAME, train_dataset, path="tmp/new_tokenizer_digits")
    #new_tokenizer = train_tokenizer_add_diff("tmp/new_tokenizer_digits", train_dataset, path="tmp/new_tokenizer_digits_and_diff")
    new_tokenizer = AutoTokenizer.from_pretrained("tmp/new_tokenizer_digits_and_diff")
    
    test_tokenizer(new_tokenizer, text, "New Tokenizer from Llama-3-8B")
    test_tokenizer(AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME), text, "Llama-3-8B")

    text = "What is the time complexity of binary search?"
    test_tokenizer(new_tokenizer, text, "New Tokenizer from Llama-3-8B")
    test_tokenizer(AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME), text, "Llama-3-8B")

if __name__ == "__main__":
    main()